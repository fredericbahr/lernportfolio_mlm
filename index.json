[{"content":"Innerhalb des Seminars haben wir uns als Gruppe mit jedem Schritt des Lebenszyklus von multimedialen Daten auseinander gesetzt und recherchiert, wie diese Schritt innerhalb unseres Medientyps Video gestaltet sind bzw. welche Vorgehensweisen in der Praxis verwendet werden. Wir konzentrierten uns hierbei auf die professionelle Videoproduktion. Die Beschränkung der Domäne hatte, reflektierend betrachtet, einige Vorteile: das Themengebiet war klar abgegrenzt und jedes Gruppenmitglied konnte gezielte Untersuchungen anstellen.\nDie Idee bzw. das Konzept, sich innerhalb der Seminare in einer Gruppe mit den einzelnen Fragestellungen zu beschäftigen und auszutauschen, bewerte ich als vorteilhaft, da so eine weitere Sicht (ausgehend von der Reflexion der Vorlesung) auf das Themengebiet erlangt werden konnte. Auch die praktischen Anwendung einiger Themen der Vorlesung, wie beispielsweise die Auseinandersetzung mit der RDFLib in Python und das Erstellen von RDF Triples innerhalb des Seminars haben mir Spaß gemacht und das erlernte Wissen vertieft. Hier sei anzumerken, dass ich die Behandlung von OpenRefine auf eine Vorlesung und eine Hälfte eines Seminars aufteilen würde, da aufgrund der komplexen Installation innerhalb der Vorlesung keine Vertiefung des Tools möglich war.\nDie zeitliche Gestaltung der Seminare empfand ich teilweise unglücklich. Ich hatte oft das Gefühl, dass entweder die Aufgabenstellungen zu offen oder zu konkret formuliert waren. Dadurch wussten wir innerhalb unserer Gruppe nicht, was explizit recherchiert oder dokumentiert werden soll. Andererseits trat bei zu konkreter Fragestellung die Problematik auf, dass die Recherche und Diskussionen frühzeitig stockten und die verbleibende Zeit mit Warten verbracht verbracht wurde. Hier muss angemerkt werden, dass wir als Gruppe Initiative hätten ergreifen können. Ebenso hatte ich das Gefühl, dass in anderen Gruppen diese Problematik nicht auftrat. Jedoch verblieb für die Besprechung der Ergebnisse der Gruppenarbeit nur wenig Zeit, sodass die Gruppen die Themen nur dürftig vorstellen konnten. Hier wäre eine bessere Aufteilung der Bearbeitungs- und Präsentationszeit von Vorteil.\nDennoch fand ich die Seminare gelungen, interaktiv und bereichernd. Ebenfalls konnte ich neue Konzepte und Technologien aufgrund Diskussionen einfach erlernen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/reflexion-seminar/","summary":"Innerhalb des Seminars haben wir uns als Gruppe mit jedem Schritt des Lebenszyklus von multimedialen Daten auseinander gesetzt und recherchiert, wie diese Schritt innerhalb unseres Medientyps Video gestaltet sind bzw. welche Vorgehensweisen in der Praxis verwendet werden. Wir konzentrierten uns hierbei auf die professionelle Videoproduktion. Die Beschränkung der Domäne hatte, reflektierend betrachtet, einige Vorteile: das Themengebiet war klar abgegrenzt und jedes Gruppenmitglied konnte gezielte Untersuchungen anstellen.\nDie Idee bzw. das Konzept, sich innerhalb der Seminare in einer Gruppe mit den einzelnen Fragestellungen zu beschäftigen und auszutauschen, bewerte ich als vorteilhaft, da so eine weitere Sicht (ausgehend von der Reflexion der Vorlesung) auf das Themengebiet erlangt werden konnte.","title":"Reflexion Seminar"},{"content":"Reflektierend betrachtet, habe ich während des Moduls viele Konzepte und Techniken des Semantic Webs kennengelernt und mich intensiv mit der Beschreibung von Ressourcen mittels RDF beschäftigt. Die Abwechslung zwischen theoretischen und praktischen Herangehensweisen an verschiedene Themen des Semantik Webs bewerte ich als gut und gelungen. Ich habe beispielsweise die typische Vorgehensweise bei der Erstellung von Open Linked bzw. FAIR Data mittels RDF kennengelernt und bin zuversichtlich, diese Prinzipien in eigenen praktischen Projekten/Veröffentlichungen umsetzen zu können. Ebenso bin ich Fähig Ontologien mithilfe von standardisierten Technologien zu definieren.\nDurch Lesen der Einträge innerhalb dieses Portfolios und Überdenken einiger Diskussionen ist mir bewusst geworden, dass die Inhalte der Vorlesung starken technischen Bezug haben und viele Konzepte/Technologien des Semantic Web lehren. Dies spiegelt sich ebenfalls in meiner Mindmap wider. Persönlich heiße ich diese Ausrichtung gut, da ich als Informatikstudent mit den verwendeten Technologien eines Fachgebietes vertraut sein sollte. Ich kann trotzdem die Beweggründe für die Gastvorträge, die meist eher eine konzeptuelle bzw. High-Level Sicht auf das Themengebiet Multimedia Lifecycle bieten, nachvollziehen, obwohl ich in den einzelnen Beiträgen oft “kritisiert” habe, dass diese geringen bzw. keinen direkten Bezug zur Vorlesung hatten. Ich denke, dass dieses Empfinden daraus resultiert, dass die Vorlesungsthemen sowie die Exkursthemen unterschiedliche Sichtweisen auf das Themengebiet vermittelt sollen. Dies war für mich nicht deutlich sichtbar, sodass eine Erwähnung dieser Trennung in einer Einführungsveranstaltung positiven Einfluss auf die strukturelle Gestaltung der Veranstaltung haben könnte.\nBesonders gelungen fand ich hingegen die didaktische Herangehensweisen von Herrn Dr. Arndt. Der regelmäßige Einsatz des Umfragetools in Big Blue Button innerhalb der Vorlesungen erachte ich als sinnvoll und gab mir die Möglichkeit des direkten Feedbacks. Zusätzlich konnte ich meine Erwartungen mit den anderen Studierenden abgleichen. Die “besondere” Prüfungsleistung im Sinne dieses Lernportfolios verkörpert neue und mir bis dato unbekannte Methoden, die zusätzlich das Schreiben von “wissenschaftlichen” Arbeiten fördert und eine angenehme Abwechslung darstellt.\nIm Ganzen betrachtet fand ich das Modul Media Lifecycle Management und die Vorlesungen sehr gut und es hat mir Spaß gemacht an diesem Modul teilzunehmen. Ich konnte mein Wissen erweitern und aufgrund des Lernportfolios dieses rekapitulieren und festigen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/reflexion-vorlesung/","summary":"Reflektierend betrachtet, habe ich während des Moduls viele Konzepte und Techniken des Semantic Webs kennengelernt und mich intensiv mit der Beschreibung von Ressourcen mittels RDF beschäftigt. Die Abwechslung zwischen theoretischen und praktischen Herangehensweisen an verschiedene Themen des Semantik Webs bewerte ich als gut und gelungen. Ich habe beispielsweise die typische Vorgehensweise bei der Erstellung von Open Linked bzw. FAIR Data mittels RDF kennengelernt und bin zuversichtlich, diese Prinzipien in eigenen praktischen Projekten/Veröffentlichungen umsetzen zu können.","title":"Reflexion Vorlesung"},{"content":" Nummer Fachbegriff Definition 1 Media Lifecycle Management Verwaltung des Lebenszyklus von Daten, insbesondere medialen Daten. Suche/Erkunden → Extraktion → Speicherung → Überarbeitung → Kopplung → Klassifikation → Qualitätsanalyse → Reparatur/Weiterentwicklung 2 Video Ein kontinuierliches (zeitabhängiges) Medium, das die Medien Bild und Audio kombiniert. Die bildliche Komponente spiegelt sich als Folge von Bildern wieder, die mit hinreichender Geschwindigkeit abgespielt werden, sodass das menschliche Auge nicht in der Lage ist, die einzelnen Bilder zu trennen. 3 Web of Data Das Web 2.0 wird so erweitert, dass Informationen auf Webseiten als semantisch strukturierte Inhalte vorliegen und eine Verlinkung zwischen den Informationen möglich ist. Dafür müssen Informationen insbesondere maschinenlesbar vorliegen und verlinkt werden können. 4 Resource Description Framework Eine wichtige fundamentale Technik zur Realisierung des Web of Data. Mithilfe von RDF können logische Aussagen zu Ressourcen definiert werden. Diese werden in Form von Triples gespeichert und können als gerichteter Graph repräsentiert werden. 5 Linked Open Data Beschreibt frei verfügbare Daten, die per URI identifizierbar sind und Verknüpfungen auf andere Ressourcen besitzen. Für die Kodierung der Daten kann RDF verwendet werden. 6 Ontologie Ontologien repräsentieren ein Schema zur Darstellung einer Menge von Begriffen und bestehender Beziehungen in einem definierten Bezugsrahmen. Ermöglichen aufgrund von Inferenz- und Integritätsregeln die Integration und Schlussfolgerung von Wissen. Weitere Definition: [\u0026hellip;] ist eine formale und explizite Spezifikation einer gemeinsamen Konzeptionierung. Sie zeichnet sich durch eine hohe semantische Ausdruckskraft aus, die für Beschreibungen mit erhöhter Komplexität benötigt wird. 7 Uniform Resource Identifier Dient als Identifikator einer Ressourcen, der in Form einer schematischen Zeichenkette repräsentiert wird. Eine URI besteht aus fünf Bestandteilen: scheme, authority, path, query, fragment. 8 URI/IRI URI steht für Uniform Resource Identifier und IRI für Internationalized Resource Identifier und dienen im Kontext RDF als eindeutige Bezeichner für Ressourcen. IRIs sind dabei eine internationalisierte Form der URIs, indem Unicode-Zeichen innerhalb des Identifikators erlaubt sind. 9 RDF-Triple RDF Triple bestehen aus einem Subjekt, einem Prädikat und einem Objekt. Zusammen bilden diese Wörter eine logische Aussage über die Ressource. Sie sind die Basis aller RDF-Technologien. 10 RDF Schema Bietet Möglichkeiten, Eigenschaften und Relationen einer Ressource zu modellieren und diese einzuschränken. Dabei werden zur Beschreibung ebenfalls RDF Triples verwendet. Alternativen zu RDF Schema sind OWL und SHACL. 12 RDF-Stack Ein anderes Wort für RDF-Stack ist der Semantik Web Stack. Dieser illustriert die wesentliche Architektur von RDF/des Semantik Webs und beinhaltet viele Prinzipien und Technologien, die in Zusammenhang mit RDF verwendet werden. 13 Serialisierung von RDF-Graphen Mithilfe der Serialisierung und verschiedenen Serialisierungsformaten können RDF-Graphen in Form von RDF-Triples syntaktisch und semantisch korrekt notiert und somit serialisiert werden. Aufgrund dieser Verwendung werden Serialsierungsformate innerhalb der Abstract Language im RDF-Stack verortet. 14 Turtle Turtle ist eines von vielen Serialsierungsformaten für RDF. Es setzt dabei den Schwerpunkt auf Lesbarkeit für den Menschen. Repetitionen von Subjekten und Prädikaten können durch eine einfache Syntax vermieden werden. 15 DBpedia Ein Projekt, welches semi-strukturierte Daten aus Wikipedia extrahiert und aus diesen RDF-Daten generiert. Ziel ist es, für Abfragen bessere Ergebnisse und aggregiertes Wissen zu liefern. 16 Wikidata Wikidata ist eine Wissensdatenbank, die frei und öffentlich ist. Sie kann sowohl von Menschen als auch von Maschinen bearbeitet werden und dient als zentraler Speicher für beispielsweise Wikipedia. 17 SPARQL SPARQL ist eine graphenbasierte Abfragesprache und dient dazu, Queries an RDF-Graphen zu senden. 18 OpenRefine Mithilfe der RDF-Extension für OpenRefine kann die Software für die Bereinigung und Transformation der Daten in RDF Triple genutzt werden. Dafür werden die Daten in einer Spalte entsprechend einem Objekt zugeordnet (Mapping). Für die Bereinigung stehen zahlreiche built-in Funktionen zur Verfügung. 19 A-Box und T-Box Die A- und T-Box kategorisieren verschiedene Statements in einem Wissensgraphen. Die A-Box beinhaltet Wissen, die die eigentliche Ressource betreffen, wohingegen die T-Box das Schema- und Domänenwissen enthält. Innerhalb der T-Box werden die Klassen und dessen Eigenschaften beschrieben. 20 Reifikation Innerhalb des RDF Schemas wird dieses Konzept für die Verdinglichung eines Statements verwendet. Dies erlaubt den Nutzer, Aussagen über eine Aussage, also ein RDF-Triple selbst, zu treffen. 21 Web Ontology Language OWL ist eine Spezifikation des W3C und dient der Erstellung von Ontologien. OWL(Full) basiert auf RDF Schema und erweitert einige der Konzepte. OWL Dokumente bestehen dabei aus Axiomen, welche ein oder mehrere RDF-Triples sind. 22 Shape Constraint Language SHACL ist eine weitere Spezifikation des W3C und dient zur Validierung von RDF-Graphen anhand einer Menge von Bedingungen. Diese Bedingungen werden über Shapses und Constraints definiert. SHACL wird dabei ebenfalls per RDF-Triples ausgedrückt. 23 Five Star Open Data Five Star Open Data ist ein 5-Sterne-Modell für offene Daten. Dieses Modell wurde von Tim Berners-Lee vorgeschlagen und kategorisiert veröffentlichte Daten nach ihrer Offenheit und Benutzbarkeit/Verbreitbarkeit. Das Modell ist dabei kumulativ, d. h. das für eine höhere Stufe die vorangegangenen erfüllt sein müssen. 24 Netzwerk-Effekt Ein Effekt, bei dem der Wert/Nutzen mit steigender Nutzerzahl ebenfalls steigt. Innerhalb des Kontextes RDF kann dies bei Verlinkungen der Fall sein. Je mehr Herausgeber die Daten verlinkbar machen und auf andere Daten verlinken, desto besser kann Wissen aggregiert und zugänglich gemacht werden. 25 Content Negotiation Ein Mechanismus innerhalb der HTTP Spezifikation, um verschiedene Versionen eines Dokumentes für die gleiche URI auszuliefern. Versionen meinen dabei beispielsweise eine RDF- oder eine HTML-Datei, zwischen der vor der Auslieferung entschieden wird. 26 FAIR Data Das Akronym FAIR steht für Findable, Accessible, Interoperable und Reusable. Daten werden als FAIR bezeichnet, wenn sie all diesen vier Grundsätzen entsprechen. 27 Virtual Research Organization Hierbei handelt es sich um eine Organisation bestehend aus verschiedenen Mitgliedern, die zu unterschiedlichen Unternehmen oder akademischen Forschungseinheiten gehören. Die Mitglieder arbeiten an einer/einem Technologie/Ziel zusammen. Für diese Organisation gelten die verschiedenen Dimensionen der verteilten Zusammenarbeit. 28 OntoWiki Es handelt sich um eine semantische Wiki-Anwendung, welche Open-Source ist. Die Software dient als Ontologie-Editor oder als Wissenserfassungssystem. 28 Datenmanagementplan Formelles Dokument, welches den Umgang mit Forschungsdaten strukturiert und beschreibt, wie während als auch nach einem Forschungsprojekt die Daten behandelt wurden und werden sollen. Er enthält unter anderem eine Projekt- sowie Datenbeschreibung, Aspekte zur Qualität und Speicherung, als auch rechtliche Aspekte. ","permalink":"https://fredericbahr.github.io/lernportfolio/posts/glossar/","summary":"Glossar für die Begriffe: Media Lifecycle Management, Video, Web of Data, Resource Description Framework, Linked Open Data, Ontologie, Uniform Resource Identifier, Interantionalized Resource Identifier, RDF-Triple, RDF Schema, RDF Stack, Serialisierung von RDF-Graphen, Turtle, DBpedia, Wikidata, SPARQL, OpenRefine, A-Box und T-Box, Reifikation, Web Ontology Language, Shape Constraint Language, Five Star Open Data, Netzwerk-Effekt, Content Negotiation, FAIR Data, Virtual Research Organization, OntoWiki, Datenmanagementplan.","title":"Glossar"},{"content":"Per Klick auf das Bild kann dieses größer betrachtet und heruntergeladen werden.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/mindmap/","summary":"Mindmap zu den Begriffen definiert im Glossar","title":"Mindmap"},{"content":"Die Aufgabe dieses Seminars bestand darin, unsere Erkenntnisse aus den vorangegangenen Seminaren zusammenzutragen und innerhalb einer Präsentation aufzubereiten. Wir überlegten zunächst, in welcher Form wir die Informationen den anderen Kursteilnehmern präsentieren wollen und entschieden uns dafür das Wiki als Grundlage der Präsentation zu wählen.\nAufgrund zeitlicher Überschneidungen verließ ich das Seminar bereits frühzeitig, kontrollierte aber am Abend noch einmal meine Einträge im Wiki.\nAls positiv bewerte ich die Möglichkeit, die Präsentation innerhalb des Seminars vorzubereiten und bereits einige Rückfragen zu organisatorischen Themen an Herrn Dr. Arndt zu stellen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-12/","summary":"Die Aufgabe dieses Seminars bestand darin, unsere Erkenntnisse aus den vorangegangenen Seminaren zusammenzutragen und innerhalb einer Präsentation aufzubereiten. Wir überlegten zunächst, in welcher Form wir die Informationen den anderen Kursteilnehmern präsentieren wollen und entschieden uns dafür das Wiki als Grundlage der Präsentation zu wählen.\nAufgrund zeitlicher Überschneidungen verließ ich das Seminar bereits frühzeitig, kontrollierte aber am Abend noch einmal meine Einträge im Wiki.\nAls positiv bewerte ich die Möglichkeit, die Präsentation innerhalb des Seminars vorzubereiten und bereits einige Rückfragen zu organisatorischen Themen an Herrn Dr.","title":"Seminar 12"},{"content":"Exkurs eccenca GmbH:\nDie eccenca Gmbh ist ein europaweit agierendes Unternehmen mit Start-Up-DNA, die durch eine Ausgliederung aus einer Forschungsgruppe entstand. Ziel ist es, die Digitalisierung in Unternehmen zu beschleunigen und zu verbessern. Dabei wird auf eine Vermarktung von Wissensgraphen gesetzt. Um die Digitalisierung zu erreichen, sollen Daten aus sogenannten Datensilos extrahiert und migriert/integriert werden. Dabei verwendet die eccenca GmbH einen sogenannten Business Digital Twin. Aufschlussreich war der Iterative Prozess, in dem zuerst die Daten analysiert und ein Domänenwissen aufgebaut wird, mithilfe dieses Wissens ein semantische Modell, bestehend aus Ontologien und Vokabularen, erstellt wird. Anschließend findet ein Mapping zwischen Daten und Ontology statt, wobei die Daten verlinkt, angereichert und bereinigt werden. Nach Reviews mit dem Kunden wird dieser Prozess wiederholt und die einzelnen Artefakte verbessert.\nIch fande diesen Vortrag von den drei Exkursen am besten, da viele Themen der Vorlesung aufgegriffen und Assoziationen zwischen der Theorie und der Praxis hergestellt wurden. Beispielsweise führt die eccenca einen ähnlichen Prozess durch, den wir mit OpenRefine verwendet haben, um Daten zu mappen und zu bereinigen. Ebenfalls beeindruckend fand ich den unterschiedlichen Tech-Stack der jeweiligen Applikationen: Java und Scala auf der einen Seite, Python und JavaScript (React) auf der anderen. Nicht zu vergessen die Technologien für RDF (SPARQL, Turtle, Triple Stores). Dies verdeutlichte noch einmal wie breit gefächert das Gebiet des Semantik Webs in der Praxis ist.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-12/","summary":"Exkurs eccenca GmbH:\nDie eccenca Gmbh ist ein europaweit agierendes Unternehmen mit Start-Up-DNA, die durch eine Ausgliederung aus einer Forschungsgruppe entstand. Ziel ist es, die Digitalisierung in Unternehmen zu beschleunigen und zu verbessern. Dabei wird auf eine Vermarktung von Wissensgraphen gesetzt. Um die Digitalisierung zu erreichen, sollen Daten aus sogenannten Datensilos extrahiert und migriert/integriert werden. Dabei verwendet die eccenca GmbH einen sogenannten Business Digital Twin. Aufschlussreich war der Iterative Prozess, in dem zuerst die Daten analysiert und ein Domänenwissen aufgebaut wird, mithilfe dieses Wissens ein semantische Modell, bestehend aus Ontologien und Vokabularen, erstellt wird.","title":"Vorlesung 12"},{"content":"Das Seminar wurde nach Rücksprache mit uns Studierenden in zwei thematische Abschnitte unterteilt. Ich persönlich bewerte dieses Vorgehen als sehr gut, da Herr Dr. Arndt auf die Bedürfnisse und Wünsche der Studierenden eingegangen ist und Rücksicht (besonders auf für mich relevante Gebiete) genommen hat. Neben einer kurzen Einführung der Software OntoWiki, in der Herr Dr. Arndt erklärte, wie Klassen, Properties und Instanzen richtig angelegt werden, haben wir in unserer Gruppe einige Objekte modelliert. Die praktische Auseinandersetzung mit der Thematik und der Software hat für mich das Wissen, gerade über den Aufbau von RDF und Ontologien, gefestigt.\nZusammen mit meiner Gruppe habe ich anschließend versucht die Frage “Wie kann ich Qualität messen?” im Kontext der professionell Videoproduktion zu beantworten. In Anbetracht der Zeit konnten lediglich die wichtigsten Qualitätsmetriken und Merkmale betrachtet werden, was persönlich ausreichend war. Ich empfand das Seminar als sehr positiv. Mir hat vor allem die Arbeit mit der Software OntoWiki Freude bereitet.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-11/","summary":"Das Seminar wurde nach Rücksprache mit uns Studierenden in zwei thematische Abschnitte unterteilt. Ich persönlich bewerte dieses Vorgehen als sehr gut, da Herr Dr. Arndt auf die Bedürfnisse und Wünsche der Studierenden eingegangen ist und Rücksicht (besonders auf für mich relevante Gebiete) genommen hat. Neben einer kurzen Einführung der Software OntoWiki, in der Herr Dr. Arndt erklärte, wie Klassen, Properties und Instanzen richtig angelegt werden, haben wir in unserer Gruppe einige Objekte modelliert.","title":"Seminar 11"},{"content":" Quizfrage: Wofür steht das Akronym FAIR und was sind die Kernelemente der einzelnen Abkürzungen? Antwort: FAIR steht für die Wörter Findable, Accessible, Interoperable und Reusable. Findable bedeutet, dass Daten mit einem eindeutigen Identifikator und umfangreichen Metadaten versehen sind. Accessible verdeutlicht, dass die Daten mittels anerkannter und standardisierten Kommunikationsprotokollen abgerufen werden können. Interoperable meint dabei, dass Daten ausgetauscht, interpretiert und mit anderen Datensätzen kombiniert werden können. Zuletzt sollten die Daten aufgrund ihrer Lizenz und Dokumentation für eine Nachnutzung geeignet sein.\nFrage: Wieso benötigen wir strukturierte Datenformate im Web?\nAntwort: Strukturierte Datenformate sind maschinenlesbar und erlauben eine automatisierte Verarbeitung. Dafür ebenfalls wichtig ist eine Standardisierung der Datenformate. Ebenfalls können damit gut komplexe Daten dargestellt werden. Die Daten sollten ausgetauscht, interpretiert und automatisiert verknüpft werden können.\nFrage: Wie können wir dafür sorgen, dass unsere Daten verlinkbar sind, warum sollten wir auf andere Daten verlinken?\nAntwort: Durch die Verwendung von URIs bzw. IRIs ermöglicht man die Verlinkung auf seine Ressourcen. Verlinkt man ebenfalls auf andere Daten, so können Nutzer einen Kontext zu seinen Informationen herstellen und durch die Verlinkung trägt man zu dem sogenannten Netzwerkeffekt bei. Ebenfalls kann durch die Verlinkung verwandtes Wissen aggregiert werden.\nIn der Vorlesungen haben wir zunächst die FAIR-Prinzipien für Daten vertieft. Anschließend haben wir betrachtet, mit welchen Hilfsmitteln Daten klassifiziert und angereichert werden können. Erstaunlich war die Erkenntnis, dass eine spezifischere Klassifikation meist auch eine Anreicherung darstellt. Mir haben diekleinen Demobeispiele zu einigen Tools gefallen, da dies die Vorlesung aufgelockerten und praktische Aspekte vermittelten.\nQuelle: https://api.thehyve.nl/uploads/FAIR-principles.png\nWeiterhin haben wir uns mit der Datenqualität beschäftigt bzw. welche Auswirkungen aus schlechter Datenqualität resultieren können und welche Metriken zur Messung der Datenqualität vorliegen. Spannend fand ich die Erläuterungen über die verschiedenen Herausforderungen und Möglichkeiten der verteilten Zusammenarbeit innerhalb der Doktorandenarbeit von Herrn Dr. Arndt. Dies ermöglichte Einblicke in das Forschungsgebiet von Herrn Dr. Arndt und verdeutlichte erneut die praktische Relevanz des Moduls.\nIch hatte das Gefühl, dass die Themen in der Vorlesung etwas unsortiert waren und wir durch mehrere Themengebiete gesprungen sind. Dies reduzierte meine Auffassungsgabe, da ich mich stets in etwas Neues hineindenken musste. Dadurch konnte ich keine visuelle Mindmap erstellen, sodass die Themen nur schwerer sortierbar waren. Eine Mindmap oder ein einleitender Überblick wäre meines Erachtens nach hilfreich.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-11/","summary":"In der Vorlesungen haben wir uns zunächst mit den FAIR-Prinzipien für Daten beschäftigt. Anschließend haben wir uns angeschaut, mit welchen Hilfsmitteln Daten klassifiziert und angereichert werden können. Dabei fand ich erstaunlich, dass eine weitere Klassifikation meist auch eine Anreicherung darstellt. Dieser Blickwinkel war für mich neu und hat mich überrascht. Ebenso empfand ich die kleinen Demobeispiele einiger Tools als gut, da dies die Vorlesung aufgelockerte und praktische Aspekte vermittelte. Weiterhin haben wir uns mit der Datenqualität beschäftigt, was einerseits eine schlechte Qualität für Auswirkungen hat und welche Metriken es für die Datenqualität gibt. Spannend fand ich die Erläuterungen zu Teilen der Doktorandenarbeit von Herrn Arndt über die Herausforderungen und Möglichkeiten der verteilten Zusammenarbeit.\nLeider hatte ich das Gefühl, dass die Vorlesung/Themen etwas unsortiert waren und wir durch mehrere Themengebiete gesprungen sind. Dies reduzierte meine Auffassungsgabe, da ich mich stets in etwas neues hineindenken musste. Dadurch konnte ich mir kein visuelles Mindmap erstellen, sodass ich die Themen schwer sortieren konnte.","title":"Vorlesung 11"},{"content":"Inhalt des Seminars waren Gruppierungs-, Einteilungs- sowie der Kategorisierungsmöglichkeiten von professionell produzierten Videos. Wir haben ebenfalls betrachtet, wie Metadaten angereichert werden können, um zusätzliche Informationen zu codieren. Neben den geläufigen Gruppierungsmöglichkeiten nach Genre oder Stilmittel haben wir uns auch damit beschäftigt, wie Filme nach dem Setting oder dem narrativen Stil kategorisiert werden können. Additional haben wir Einteilungsmöglichkeiten wie Altersfreigabe und Regisseure oder andere Metadaten identifizieren können. Informativ war die Recherche zu einer Software, die die einzelnen Frames eines Videos analysiert und eine aggregierte Visualisierung zu verschiedenen Bildmetriken der einzelnen Frames darstellt.\nDie Untersuchung der Arten der Anreicherung ist uns zunächst schwer gefallen, jedoch konnte mithilfe von Herr Dr. Arndt verschiedene Techniken gefunden werden: Beispielsweise können Graphdatenbanken, wie imdb.de das realisiert hat, verwendet oder RDF-Graphen entwickelt werden. Zusätzlich können die Metadatenformate verändert und erweitert werden.\nAls negativ bewerte ich die viele Zeit, die uns zur Verfügung stand. Nach knapp einer Stunde war unsere Gruppe (auch aufgrund von lediglich drei Personen) mit der Recherche/Diskussion fertig, sodass wir die verbleibende Zeit mit Warten verbrachten, bis die einzelnen Ergebnisse besprochen wurden. Die Besprechungszeit war jedoch dann wiederum zu kurz, sodass wir überziehen mussten und unsere Ergebnisse nur sehr knapp vorstellen konnten. Hier wäre ein besseres Zeitmanagement oder Abfragen zur noch benötigten Zeit vorteilhaft.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-10/","summary":"Inhalt des Seminars waren Gruppierungs-, Einteilungs- sowie der Kategorisierungsmöglichkeiten von professionell produzierten Videos. Wir haben ebenfalls betrachtet, wie Metadaten angereichert werden können, um zusätzliche Informationen zu codieren. Neben den geläufigen Gruppierungsmöglichkeiten nach Genre oder Stilmittel haben wir uns auch damit beschäftigt, wie Filme nach dem Setting oder dem narrativen Stil kategorisiert werden können. Additional haben wir Einteilungsmöglichkeiten wie Altersfreigabe und Regisseure oder andere Metadaten identifizieren können. Informativ war die Recherche zu einer Software, die die einzelnen Frames eines Videos analysiert und eine aggregierte Visualisierung zu verschiedenen Bildmetriken der einzelnen Frames darstellt.","title":"Seminar 10"},{"content":" Quizfrage: Wofür stehen die “Five Star Open Data”?\nAntwort: Die Five Star Open Data bilden eine hierarchische Kategorisierungsgruppe nach denen Dateiformate bzw. Technologien hinsichtlich ihrer Güte für eine Verlinkung von Ressourcen kategorisiert werden können.\nFrage: Was sind die wichtigsten Merkmale um den Begriff Ontologie einordnen zu können?\nAntwort: Eine Ontologie definiert und beschreibt Ausdrücke, modelliert eine Domäne, basiert auf einem Konsens, ist maschienenlesbar und besitzt eine hohe Aussagekraft bei steigender Komplexität.\nFrage: Was sind RDF-S, OWL und SHACL und wozu kann es jeweils verwendet werden?\nAntwort: RDF-S = RDF-Schema, OWL = Web Ontology Language, SHACL = Shapes Constraint Language RDF Schema ist ein Teil von RDF und wird in RDF beschrieben. Es verfolgt einen objektorientierter Ansatz (Vererbung, Eigenschaften/Methoden). OWL (Full) basiert auf RDF Schema und fügt weitere Beschreibungsmöglichkeiten hinzu (Transitivität, Kardinalität). SHACL ist eine Sprache zur Validierung von RDF-Graphen anhand spezifizierter Bedingungen, die in einem Shape-Modell mit Constraints per RDF beschrieben werden.\nDie Vorlesung behandelte die Thematik Linked Data und Verlinkung von Ressourcen. Dabei habe ich neben den Kernelementen vor allem die fünf Sterne der Open Data kennengelernt, die von Tim Berners-Lee definiert wurden. Ebenso haben wir uns dem Thema Content Negotiation genähert.\nBesonders überrascht hat mich, dass proprietäre Dateiformate und einfache Dateiformate wie CSV schon unter dem Gesichtspunkt von Linked Data betrachtet werden können. Für mich persönlich war ebenfalls ein Kerngewinn dieser Vorlesung, dass die Vorteile der Verlinkung für die Maschinenenlesbarkeit und Verknüpfung/Aggregation von Daten einen großen Nachteil mit sich zieht: Die Erstellung solcher Informationen/Daten beansprucht deutlich mehr Zeit und ist meist komplexer als die Verwendung von proprietären Dateiformaten oder unstrukturierten Daten, sodass eine Abwägung erfolgen muss.\nMich würde weiterhin interessieren, welche Software bereits veröffentlicht wurde, um die Erstellung solcher verlinkter Daten zu generieren und ob diese benutzerfreundlich für eine breite Masse an Internetnutzern ist. Dies wäre aus meiner Sicht ein guter Ausgangspunkt für weitere Recherche oder Projekte.\nDas Konzept der Content Negotiation kannte ich bereits. Ich empfand den Zeitpunkt der Erläuterungen als etwas obskur, da wir in vorherigen Beispielen bereits die Content Negotiation erlebt haben und somit entweder früher Fragen aufgeworfen hat oder zum Zeitpunkt dieser Vorlesung repetitiv war. Dennoch sind nähere Erläuterungen zu diesem Konzept sinnvoll, um beispielsweise RDF-Daten über HTTP abzurufen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-10/","summary":"Die Vorlesung behandelte die Thematik Linked Data und Verlinkung von Ressourcen. Dabei habe ich neben den Kernelementen vor allem die fünf Sterne der Open Data kennengelernt, die von Tim Berners-Lee definiert wurden. Ebenso haben wir uns als Kurs dem Thema Content Negotiation genähert.\nBesonders überrascht hat mich, dass proprietäre Dateiformate und Standarddateiformate wie CSV schon unter dem Gesichtspunkt von Linked Data betrachtet werden können. Für mich persönlich war ebenfalls ein Kerngewinn dieser Vorlesung, dass die Vorteile der Verlinkung für die Maschinenenlesbarkeit und Verknüpfung/Aggregation von Daten einen großen Nachteil mit sich zieht: Die Erstellung solcher Informationen/Daten beansprucht deutlich mehr Zeit und ist meist komplexer als die Verwendung von proprietären Dateiformaten oder unstrukturierten Daten.\nMich würde weiterhin interessieren, welche Software es bereits gibt, um die Erstellung solcher verlinkter Daten zu generieren und ob diese für die breite Masse an Internetnutzern benutzerfreundlich ist. Dies wäre aus meiner Sicht ein guter Ausgangspunkt für weitere Recherche oder Projekte. Die Content Negotiation war mir bereits bekannt, leider fand ich die Stelle in der Vorlesung etwas fragwürdig, da wir in vorherigen Beispielen in diesem Modul bereits die Content Negotiation erlebt haben und dies entweder früher einige Fragezeichen aufgeworfen hat oder zum Zeitpunkt dieser Vorlesung Wiederholung war.","title":"Vorlesung 10"},{"content":"Im Rahmen des Seminars beschäftigte sich meine Gruppe mit Verlinkungsansätzen des Medientypens Video. Zusätzlich untersuchten wir, wie Ähnlichkeiten und Beziehungen zwischen verschiedenen Videos erkannt und ausgedrückt werden können. Zudem recherchierten wir, wie durch Verschmelzungen von mehreren medialen Objekten eine Art Kunstform entstehen kann. Dabei stellten wir fest, dass die Musik bzw. die einzelnen Bilder eines Videos Gemeinsamkeiten mit anderen Videos aufweisen können (Melodie, Tempo/Pixelwert). Aber auch der Inhalt von verschiedenen Videos kann ähnlich oder gleich sein. Zusätzlich gibt es Algorithmen für die Mustererkennung, Präferenzermittlung und Erfolgsaussichten (siehe bspw. Netflix).\nAufgrund der aktuellen technischen Möglichkeiten gibt es jedoch auch verschiedene Kunstformen mit dessen Hilfe der Inhalt, beispielsweise durch Deep Fakes, verändert werden kann. Jedoch bieten auch Parodien, Crossovers, oder Neuverfilmungen die Möglichkeit eine Beziehung/Verlinkung zwischen verschiedenen Medienobjekten herzustellen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-9/","summary":"Im Rahmen des Seminars beschäftigte sich meine Gruppe mit Verlinkungsansätzen des Medientypens Video. Zusätzlich untersuchten wir, wie Ähnlichkeiten und Beziehungen zwischen verschiedenen Videos erkannt und ausgedrückt werden können. Zudem recherchierten wir, wie durch Verschmelzungen von mehreren medialen Objekten eine Art Kunstform entstehen kann. Dabei stellten wir fest, dass die Musik bzw. die einzelnen Bilder eines Videos Gemeinsamkeiten mit anderen Videos aufweisen können (Melodie, Tempo/Pixelwert). Aber auch der Inhalt von verschiedenen Videos kann ähnlich oder gleich sein.","title":"Seminar 9"},{"content":" Quizfrage: Welche Möglichkeiten/Sprachen/Formate bietet der RDF-Stack um eine Ontologie zu beschreiben.\nAntwort: Ontologien werden ebenfalls als Triple beschrieben. Dabei bietet RDF Schema, Web Ontology Language (OWL) und die Shape Constraint Language (SHACL) ausdrucksstarke Möglichkeiten eine Ontology zu beschreiben\nFrage: Wozu dient Open Refine?\nAntwort: OpenRefine ist ein Programm, mit dessen Hilfe große Datenmengen durchsucht, bereinigt, in andere Formate transformiert und mit anderen Datenmengen verknüpft werden können. Folgende Datenformate werden beispielsweise unterstützt: CSV, XLS, JSON oder XML\nFrage: Was ist die Europeana und in welcher Verbindung steht die DDB zu Europeana?\nAntwort: Europeana ist eine virtuelle Bibliothek, die das wissenschaftliche und kulturelle Erbe Europas in Form von multimedialen Daten zugänglich machen soll.\nDie DDB ist dabei der deutsche Aggregator für Europeana und übermittelt Metadaten und Objekte aus deutschen Kultureinrichtungen. Da sich die Aufgaben der beiden Institutionen sehr ähneln, wird Europeana als die große Schwester von der DDB angesehen.\nInnerhalb dieser Vorlesung haben wir uns dem Thema Ontologie gewidmet und von Herr Dr. Arndt erfahren, wie Ontologien innerhalb des RDF-Stacks beschrieben werden können. Ontologien beschreiben dabei eine formale, explizite Spezifikation einer gemeinsamen Konzeptionierung, die durch eine hohe semantische Ausdruckskraft charakterisiert ist. Durch Ontologien kann man den Triple-Graphen in zwei Boxen/Kategorien (T-Box und A-Box) gliedern. Zusätzlich kann in RDF Schema mithilfe eines vergleichbaren Modells wie in der objekt-orientierten Programmierung eine Hierarchie aufgebaut werden (Vererbung, Eigenschaften/Methoden, etc.). RDF-Container ermöglichen dabei eine Reifikation.\nOWL basiert auf RDF Schema und fügt einige weitere Beschreibungsmöglichkeiten (Transitivität, Kardinalität, etc.) hinzu.\nSHACL hingegen verfolgt ein anderes Modell, was auf Basis eines Shape-Modells mit Constraints ausgelegt ist.\nAn der Vorlesung hat mir gut gefallen, dass wir weitere Möglichkeiten von RDF in bezug auf die Beschreibung von Ontologien kennengelernt haben. Leider war die verfügbare Zeit gering, sodass aus meiner Perspektive praktische Anwendungen und Übungsmöglichkeiten nicht vermittelt werden konnten. Wünschenswert wären hier praktische Aufgaben in einem Folgeseminar.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-9/","summary":"Innerhalb dieser Vorlesung haben wir uns dem Thema Ontologie gewidmet und von Herr Dr. Arndt erfahren, wie Ontologien innerhalb des RDF-Stacks beschrieben werden können. Ontologien beschreiben dabei eine formale, explizite Spezifikation einer gemeinsamen Konzeptionierung, die durch eine hohe semantische Ausdruckskraft charakterisiert ist. Durch Ontologien kann man den Graph von Triples in zwei Boxen/Kategorien (T-Box und A-Box) aufteilen. In RDF Schema kann dann mithilfe eines ähnlichen Modells wie in der objekt-orientierten Programmierung eine Hierarchie aufgebaut werden (Vererbung, Eigenschaften/Methoden, etc.). RDF-Container ermöglichen dabei dann eine Reifikation. OWL basiert auf RDF Schema und fügt einige weitere Beschreibungsmöglichkeiten (Transitivität, Kardinalität, etc.) hinzu. SHACL hingegen verfolgt ein anderes Modell, was auf Basis eines Shape-Modells mit Constraints ausgelegt ist.\nAn der Vorlesung hat mir gut gefallen, dass wir weitere Möglichkeiten von RDF in bezug auf die Beschreibung von Ontologien kennengelernt haben. Leider war die Zeit recht knapp, sodass für mich persönlich wichtige Übungen/praktische Anwendungen nicht vermittelt werden konnten. Wünschenswert hier wären praktische Aufgaben in einem nachfolgenden Seminar.","title":"Vorlesung 9"},{"content":" Aufgrund von Krankheit konnte ich leider nicht teilnehmen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-8/","summary":"Aufgrund von Krankheit konnte ich leider nicht teilnehmen.","title":"Seminar 8"},{"content":"Exkurs Deutsche Digitale Bibliothek:\nIn dieser Vorlesung hat Frau Bertha einen Gastvortrag über die Deutsche Digitale Bibliothek gehalten. Zentraler Inhalt des Vortrages war die Charakterisierung der DDB, insbesondere als Aggregator für Europeana und die Bereitstellungen von Diensten. Die Erklärungen zur “unmöglichen Trinität von Metadaten” waren sehr aufschlussreich und hatte einen konkreten Bezug zum Themengebiet der Vorlesung, da wir uns ebenfalls mit Metadaten auseinandergesetzt haben. Leider konnte ich dem Vortrag aufgrund der veralteten Darstellung der Präsentation sowie der späten Veranstaltung nur mühselig folgen. Aufgrund dieser Tatsache nehme ich mir für die darauffolgenden Vorlesungen/Seminare vor, konzentrierter und auch aktiver an der Lehrveranstaltung teilzunehmen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-8/","summary":"Exkurs Deutsche Digitale Bibliothek:\nIn dieser Vorlesung hat Frau Bertha einen Gastvortrag über die Deutsche Digitale Bibliothek gehalten. Zentraler Inhalt des Vortrages war die Charakterisierung der DDB, insbesondere als Aggregator für Europeana und die Bereitstellungen von Diensten. Die Erklärungen zur “unmöglichen Trinität von Metadaten” waren sehr aufschlussreich und hatte einen konkreten Bezug zum Themengebiet der Vorlesung, da wir uns ebenfalls mit Metadaten auseinandergesetzt haben. Leider konnte ich dem Vortrag aufgrund der veralteten Darstellung der Präsentation sowie der späten Veranstaltung nur mühselig folgen.","title":"Vorlesung 8"},{"content":"Unsere Arbeitsgruppe hat in diesem Seminar die Möglichkeiten der Bearbeitung in der Postproduktion von Filmen recherchiert. Ebenfalls haben wir uns damit beschäftigt, welche (kommerzielle) professionelle Bearbeitungssoftware für die Postproduktion erhältlich ist. Abschließend untersuchten wir die typischen Bearbeitungsschritten in der Postproduktion und identifizierten eine geeignete graphische Darstellung des Workflows.\nInteressant war die Recherche und der Austausch mit den Gruppenmitgliedern über die typischen Bearbeitungsschritte bzw. -reihenfolge. Hier gab es einige kontroverse Meinungen, die sich nach gemeinsamer Nachforschung allesamt als richtig erwiesen haben. Unbekannt waren mir die Möglichkeiten der Softwares zur parallelen Verarbeitung in unterschiedlichen Teams.\nEbenso habe ich gelernt, dass auch die Hollywood-Filmproduktion viele verschiedene Subkategorien aufweist und für diese spezielle Tools und Workflows verwendet werden müssen. So entstand die Debatte, ob die vollständig digitale Erstellung und Bearbeitung von Cartoons als Manual revision/Authoring klassifiziert werden kann, da es sich um keine klassische Bearbeitung, sondern vielmehr um eine Erzeugung handelt.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-7/","summary":"Unsere Arbeitsgruppe hat in diesem Seminar die Möglichkeiten der Bearbeitung in der Postproduktion von Filmen recherchiert. Ebenfalls haben wir uns damit beschäftigt, welche (kommerzielle) professionelle Bearbeitungssoftware für die Postproduktion erhältlich ist. Abschließend untersuchten wir die typischen Bearbeitungsschritten in der Postproduktion und identifizierten eine geeignete graphische Darstellung des Workflows.\nInteressant war die Recherche und der Austausch mit den Gruppenmitgliedern über die typischen Bearbeitungsschritte bzw. -reihenfolge. Hier gab es einige kontroverse Meinungen, die sich nach gemeinsamer Nachforschung allesamt als richtig erwiesen haben.","title":"Seminar 7"},{"content":" Quizfrage: Wozu kann OpenRefine einerseits verwendet werden?\nAntwort: OpenRefine mit der RDF-Extension kann dafür verwendet werden Datensätze einzulesen, zu bearbeiten und zu bereinigen. Zusätzlich können dann die Daten per Mapping in RDF Triples übersetzt werden und so einen RDF-Graphen erstellt werden.\nFrage: Was ist DBpedia und warum nutzt man DBpedia?\nAntwort: DBPedia extrahiert Daten aus Wikipedia und stellt diese strukturiert als Linked Data zur Verfügung. Dadurch werden diese Informationen einerseits maschinell abrufbar, andererseits können komplexe Suchanfragen gestellt werden.\nFrage: Nennen Sie eine wesentliche Gemeinsamkeit und einen Unterschied von DBpedia und Wikidata.\nAntwort: Gemeinsamkeiten:\nVersuchen Informationen aus Wikipedia strukturiert zu speichern Abfrage per SPARQL Unterschiede:\nAufbau der strukturierten Informationen (RDF vs. JSON, XML, SQL) Repräsentation von Fakten (Triple vs. Statements und Claim) DBpedia extrahiert Daten/Informationen Wikidata erhält Informationen über Eingabe durch Autor Im Rahmen der Vorlesung haben wir uns mit dem Tool OpenRefine beschäftigt, um Daten zu bearbeiten, zu bereinigen oder zu entfernen. Verwunderlich war die komplizierte Installation von OpenRefine unter Windows, bei der eine ZIP-Datei gedownloadet und entpackt werden muss. Ebenfalls erforderlich war es die RDF-Extension im richtigen Ordner, mit korrektem Namen abzulegen. Hier wäre eine Installation per Wizard oder per CLI wünschenswert. Aufgrund dieser organisatorischen Hürde blieb innerhalb der Vorlesung wenig Zeit für die praktische Auseinandersetzung mit dem Tool. Jedoch konnte ich das Potenzial von OpenRefine erkennen und war überrascht von der Anzahl an built-in Methoden, um Daten zu bearbeiten und zu bereinigen. Auch die Verwendung des RDF-Plugins um Tabellen (Daten) in RDF-Triple umzuwandeln, war hilfreich und verdeutlichte die Einsatzmöglichkeiten dieses Tools in der Praxis. Der mangelnden Zeit schulden will/müsste ich mich aber noch intensiver mit diesem Tool beschäftigen.\nNachtrag: Die weitere Auseinandersetzung hat sich als praktisch erwiesen, da so im Vortrag der essenca GmbH einige Parallelen zwischen OpenRefine und essencas Extraktion von Daten und Generierung von RDF Triples erwiesen hat.\nQuelle: https://multimedia.journalism.berkeley.edu/wp-content/uploads/openrefine0.png\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-7/","summary":"In dieser Vorlesung haben wir uns mit dem Tool OpenRefine beschäftigt, um Daten zu bearbeiten, zu bereinigen oder zu entfernen. Überrascht hat mich die doch komplizierte Installation von OpenRefine unter Windows, wo eine ZIP-Datei gedownloadet und entpackt werden muss und die RDF-Extension im richtigen Ordner, mit richtigem Namen abzulegen war. Hier wäre eine Installation per Wizard oder per CLI wünschenswert. Leider hat dieser Teil auch den Großteil der Vorlesung in Anspruch genommen, sodass wenig Zeit für die praktische Auseinandersetzung mit dem Tool an sich blieb. Jedoch konnte ich das Potenzial von OpenRefine erkennen und war überrascht von der Anzahl an built-in Methoden, um die Daten zu bearbeiten und zu bereinigen. Auch die Verwendung des RDF-Plugins um die Tabelle (Daten) in RDF-Triple umzuwandeln, war hilfreich und verdeutlichte die Einsatzmöglichkeiten dieses Tools in der Praxis. Der mangelnden Zeit schulden will/müsste ich mich aber noch intensiver mit diesem Tool beschäftigen.\u0026quot;","title":"Vorlesung 7"},{"content":"Innerhalb des Seminars hat sich unsere Gruppe mit den Fragen rund um die verschiedenen Datenformate, die Erfassung und Extraktion von Metadaten sowie spezielle Mediendatenbanken für unser Medium Video beschäftigt. Dabei habe ich mich einerseits mit den verschiedenen generischen Kompressionierungsverfahren für Videos und mit der Oracle Multimedia Datenbank sowie SQL/MM studiert. Erwähnenswert dabei sind die verschiedenen Möglichkeiten ein Video zu komprimieren: Es können nur die Differenzen zwischen Einzelbildern oder ein Bewegungsvektor für das Verschieben eines Bildelementes innerhalb des Bildes gespeichert werden . Zudem ermöglicht die Entropiekodierung eine derartige Kodierung von Informationen, sodass häufig auftretende Informationen mit möglichst kleinen Codewörtern kodiert werden um Speicherplatz zu sparen.\nEs hat sich zusätzlich herausgestellt, dass mithilfe von Oracle Multimedia verschiedene Metadaten automatisiert beim Einfügen einer Videodatei extrahiert werden und diese mit einer einfachen SQL-Abfrage abgefragt werden können.\nGut umsetzen konnte ich mein Vorhaben erst genau zu recherchieren bevor ich mein Wissen mit den anderen geteilt habe. Daran möchte ich weiter anknüpfen.\nAn der inhaltlichen sowohl strukturellen Gestaltung des Seminares ist nichts zu kritisieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-6/","summary":"Innerhalb des Seminars hat sich unsere Gruppe mit den Fragen rund um die verschiedenen Datenformate, die Erfassung und Extraktion von Metadaten sowie spezielle Mediendatenbanken für unser Medium Video beschäftigt. Dabei habe ich mich einerseits mit den verschiedenen generischen Kompressionierungsverfahren für Videos und mit der Oracle Multimedia Datenbank sowie SQL/MM studiert. Erwähnenswert dabei sind die verschiedenen Möglichkeiten ein Video zu komprimieren: Es können nur die Differenzen zwischen Einzelbildern oder ein Bewegungsvektor für das Verschieben eines Bildelementes innerhalb des Bildes gespeichert werden .","title":"Seminar 6"},{"content":" Quizfrage: Welchen Vorteil bietet DBpedia gegenüber Wikipedia und wie wird dieser Vorteil erzielt?\nAntwort: DBpedia ermöglicht es, komplexere Suchanfragen zu beantworten und verknüpfte Aussagen über Wissen zu treffen. Dies ist möglich, indem DBpedia semi-strukturelle Informationen aus Wikipedia extrahiert und in RDF-Triples umwandelt, wodurch eine Verknüpfung von Informationen stattfindet.\nFrage: Wie ist die Struktur eines Triples in RDF?\nAntwort: Ein Triple besteht immer aus Subjekt, Prädikat und Objekt. Diese werden über URIs bzw. IRIs angegeben. Objekte können zudem entweder leere Knoten oder Literale sein.\nFrage: Wie ist der Aufbau von URIs?\nAntwort: Eine URI besteht aus fünf Bestandteilen: scheme, authority, path, query und fragment. Das häufigste Schema im Web ist dabei http bzw. https.\nDie Vorlesung behandelte das Thema DBpedia. Dabei konnte ich lernen, dass es das Ziel von DBpedia ist, die semi-strukturierten Daten von Wikipedia zu extrahieren und in strukturierte Daten zu transformieren, um komplexere Suchanfragen zu realisieren. Die Erkenntnis, dass Wikipedia bereits aus semi-strukturierten Informationen bzw. Daten besteht und ein Grundgerüst für die Artikel vorgibt, war für mich neu. Ebenfalls erstaunlich waren die von Herr Dr. Arndt angegebenen Zahlen/Metriken zu der Wissensdatenbank sowie die eigens von DBpedia erstellte Ontologie. Die von DBpedia benutze Pipeline zur Erstellung dieses Wissens setzt dabei auf neue Technologien und Qualitätssicherungen, die mir ebenfalls aus den praktischen Tätigkeiten als Werksstudent geläufig waren, sodass eine Verknüpfung aus Theorie und Praxis zu erkennen war.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-6/","summary":"Die Vorlesung ging über das Thema DBpedia. Dabei habe ich gelernt, dass es das Ziel von DBpedia ist, die semi-strukturierten Daten von Wikipedia zu extrahieren und in strukturierte Daten zu überführen, um komplexere Suchanfragen zu realisieren. Für mich neu war, dass Wikipedia bereits aus semi-strukturierten Informationen bzw. Daten besteht und ein Grundgerüst für die Artikel vorgibt. Ebenfalls erstaunlich waren die von Herr Dr. Arndt angegebenen Zahlen/Metriken zu der Wissensdatenbank sowie die eigens von DBpedia erstellte Ontologie. Die von DBpedia benutze Pipeline zur Erstellung dieses Wissens setzt dabei auf neue Technologien und Qualitätssicherungen, die mir ebenfalls aus den praktischen Tätigkeiten als Werksstudent geläufig waren, sodass eine Verknüpfung aus Theorie und Praxis zu erkennen war.","title":"Vorlesung 6"},{"content":"In diesem Seminar hat uns Herr Dr. Arndt gezeigt, wie mithilfe einer Python-Bibliothek (RDFLib) RDF-Dateien geparset und so eine Graph-Datenstruktur aufgebaut werden kann. Diese Bibliothek kann die erzeugte Graph-Datenstruktur in verschiedene Formate serialisieren und somit persistent auf einem Speichermedium speichern.\nZudem lernte ich, welche Prefixe für URIs existieren und welche Tools zur Erkundung dieser Prefixe geeignet sind. Anschließend erzeugte ich in einer Gruppenarbeit selber einen RDF-Graphen.\nDas gesamte Seminar hat mein Verständnis für die Erstellung von Triples mithilfe von RDF verstärkt. Ebenso ermöglichte es mir, die Syntax sowie Semantik des Serialisierungsformates Turtle besser zu verstehen. Besonders wertvoll für das Verständnis war die interaktive Gestaltung des Seminars trotz einer rein digitalen Veranstaltung. Insbesondere der Einsatz des Umfragewerkzeuges in Big Blue Button schätzte ich sehr.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-5/","summary":"In diesem Seminar hat uns Herr Dr. Arndt gezeigt, wie mithilfe einer Python-Bibliothek (RDFLib) RDF-Dateien geparset und so eine Graph-Datenstruktur aufgebaut werden kann. Diese Bibliothek kann die erzeugte Graph-Datenstruktur in verschiedene Formate serialisieren und somit persistent auf einem Speichermedium speichern.\nZudem lernte ich, welche Prefixe für URIs existieren und welche Tools zur Erkundung dieser Prefixe geeignet sind. Anschließend erzeugte ich in einer Gruppenarbeit selber einen RDF-Graphen.\nDas gesamte Seminar hat mein Verständnis für die Erstellung von Triples mithilfe von RDF verstärkt.","title":"Seminar 5"},{"content":"Vorlesung anstatt Seminar: Quizfrage: Welche Vorteile bieten die Serialisierungsformate N-Triples, Turtle und RDF/XML?\nAntwort: Das Serialisierungsformat N-Triples legt den Schwerpunkt auf einfaches Parsing, wohingegen das Serialisierungsformat Turtle eine einfachere Lesbarkeit für Menschen realisieren will. Zudem ermöglichen verkürzte Schreibweisen eine Reduktion der Implementierungszeit sowie kleinere Textdateien. RDF/XML ist historisch gewachsen und war das erste Standardserialisierungsformat. Alle drei Formate sind zusätzlich Empfehlungen des W3C.\nNach einer kurzen Wiederholung der letzten Vorlesung sowie einer Einordnung des Themas RDF Serialisierung, habe ich zunächst die verschiedenen Serialisierungsformate (N3, Turtle, NTriples, JSON-LD, etc.) sowie deren Mächtigkeiten kennengelernt. Erstaunt hat mich die große Anzahl an Serialisierungsformate, die zur Beschreibung von RDF zur Verfügung stehen.\nQuelle: https://www.w3.org/2013/dwbp/wiki/images/thumb/1/17/RDFSerialization-formats.png/800px-RDFSerialization-formats.png\nAls hilfreich empfand ich dabei die bildliche Beschreibung des Transformationsprozesses von einem Tripel zum Graphen zur tabellarischen Darstellung. Dies hat mir beim Verstehen der N-Triples/N-Quads aber auch der Turtle Syntax geholfen. Am Ende der Vorlesung habe ich die verschiedenen Syntaxabkürzungen (Komma, Semikolon, Prefix), die Turtle bereitstellt und die Vorteile/Nachteile dieses Serialisierungsformates auseinandergesetzt.\nMein Wunsch und Hoffnungen ist es, dass wir uns in einem nachfolgenden Seminar noch praktisch mit dieser Thematik auseinandersetzen. Ausgehend von einem späteren Standpunkt weiß ich, dass diese Hoffnung erfüllt wurde.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-4/","summary":"Vorlesung anstatt Seminar: Quizfrage: Welche Vorteile bieten die Serialisierungsformate N-Triples, Turtle und RDF/XML?\nAntwort: Das Serialisierungsformat N-Triples legt den Schwerpunkt auf einfaches Parsing, wohingegen das Serialisierungsformat Turtle eine einfachere Lesbarkeit für Menschen realisieren will. Zudem ermöglichen verkürzte Schreibweisen eine Reduktion der Implementierungszeit sowie kleinere Textdateien. RDF/XML ist historisch gewachsen und war das erste Standardserialisierungsformat. Alle drei Formate sind zusätzlich Empfehlungen des W3C.\nNach einer kurzen Wiederholung der letzten Vorlesung sowie einer Einordnung des Themas RDF Serialisierung, habe ich zunächst die verschiedenen Serialisierungsformate (N3, Turtle, NTriples, JSON-LD, etc.","title":"Seminar 4"},{"content":"Exkursion Sächsisches Staatsarchiv:\nFrau Dr. Kluttig kommunizierte in ihrem Vortrag, dass das Staatsarchiv Leipzig verschiedene Dokumente von diversen Behörden (Reichs-, Bundes, Verwaltungs- und Justizbehörden), aber auch von politischen Parteien, insbesondere der SED, archiviert. Besonders überrascht hat mich die Sicht auf den Lebenszyklus und den Zweck von Dokumenten. Dokumente können dabei einen Primärzweck (der Zweck für den das Dokument erstellt wurde) und einen Sekundärzweck (differenten Zweck nach Primärzweck) aufweisen. Ebenso fängt durch die Archivierung des Dokumentes ein zweiter Lebenszyklus statt. Erstaunlich war ebenfals der bzw. die Prozesse die durchlaufen werden, bis ein Dokument archiviert oder digitalisiert wird. Dabei habe ich gelernt, dass aus Sicht eines Archives die Digitalisierung von Dokumenten nicht immer vorteilhaft ist und Originalquellen präzisere Aussagen/Auswertungen erlauben als die digitalen Kopien. Die Einhaltungen von Datenschutzgeschetzen, obwohl die eigentlich betroffene Person bereits verstorben ist, hat mich ebenfalls verwundert. Der direkte Bezug zu Vorlesungsinhalten war für mich jedoch nicht offensichtlich, da dieser meines Erachtens nach sehr technisch ausgelegt ist. Frau Dr. Kluttig berichtete größtenteils von analogen Verfahren und Prozessen, wohingegen wir uns mit digitalen Technologien auseinandersetzen. Nachtrag: Nach weiteren Diskussionen mit Herr Dr. Arndt habe ich erkannt, dass die Vorträge nicht inhaltliche Themen vertiefen sondern andere Sichten ermöglichen/vermitteln sollen. Um den Überblick besser herstellen zu können, wäre in der Zukunft eine deutliche Kommunikation der Rollen der Vorträge von Vorteil.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-4/","summary":"Exkursion Sächsisches Staatsarchiv:\nFrau Dr. Kluttig kommunizierte in ihrem Vortrag, dass das Staatsarchiv Leipzig verschiedene Dokumente von diversen Behörden (Reichs-, Bundes, Verwaltungs- und Justizbehörden), aber auch von politischen Parteien, insbesondere der SED, archiviert. Besonders überrascht hat mich die Sicht auf den Lebenszyklus und den Zweck von Dokumenten. Dokumente können dabei einen Primärzweck (der Zweck für den das Dokument erstellt wurde) und einen Sekundärzweck (differenten Zweck nach Primärzweck) aufweisen. Ebenso fängt durch die Archivierung des Dokumentes ein zweiter Lebenszyklus statt.","title":"Vorlesung 4"},{"content":"Zu Beginn des Seminars wurden die einzelnen Gruppenergebnisse des letzten Seminars kurz vorgestellt. Anschließen haben wir in einer Diskussionsphase die Fragestellung beantwortet, welche Datenquellen und -sätze für unseren Medientypen Video existieren. Geschätzt habe ich den lockeren Austausch von vorhandenem Wissen mit den Kommilitonen und die Erkenntnis, dass wir uns innerhalb der Bearbeitung auf eine spezielle Einsatzdomäne beschränken wollen.\nGewählt wurde die Domäne der professionellen Videoproduktion. Das Bestreben resultierend aus dem letzten Seminar, erst zu recherchieren und das Wissen zu validieren, konnte ich innerhalb dieses Seminars bereits umsetzen. Die Fragestellung war aus persönlicher Sicht etwas zu detailliert gestellt, sodass nach einer halbstündigen Bearbeitung die Ergebnisse feststanden und die weitere Bearbeitungszeit für mich/uns überflüssig war.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-3/","summary":"Zu Beginn des Seminars wurden die einzelnen Gruppenergebnisse des letzten Seminars kurz vorgestellt. Anschließen haben wir in einer Diskussionsphase die Fragestellung beantwortet, welche Datenquellen und -sätze für unseren Medientypen Video existieren. Geschätzt habe ich den lockeren Austausch von vorhandenem Wissen mit den Kommilitonen und die Erkenntnis, dass wir uns innerhalb der Bearbeitung auf eine spezielle Einsatzdomäne beschränken wollen.\nGewählt wurde die Domäne der professionellen Videoproduktion. Das Bestreben resultierend aus dem letzten Seminar, erst zu recherchieren und das Wissen zu validieren, konnte ich innerhalb dieses Seminars bereits umsetzen.","title":"Seminar 3"},{"content":" Frage: Wie kann mithilfe von RDF eine Ressource beschrieben werden und welche Eigenschaften besitzt diese Beschreibungsmöglichkeit?\nAntwort: RDF beschreibt mithilfe eines Triples eine Ressource. Das Triples bildet dabei eine logische Aussage durch die Verknüpfung eines Prädikats mit einem Subjekt und Objekt. Triples können als gerichteter Graph dargestellt werden.\nFrage: Welchen Nutzen hätte ein umfassendes Semantic Web?\nAntwort: Es bietet durch auffindbare Informationen Transparenz. Informationen liegen maschinenlesbar vor, sodass sie automatisiert integriert werden können. Ebenso fördert es den Austausch von Daten über das Web.\nFrage: Wie unterscheiden sich Web 1.0, Web 2.0 \u0026amp; Web 3.0?\nAntwort: Das Web 1.0 umfasst Dokumente von Institutionen für eine geringe Anzahl an Nutzern, wohingegen das Web 2.0 durch benutzergenerierte Informationen und eine Zentralisierung hin zu großen Services charakterisiert werden kann. Das Web 3.0 schließlich verfolgt eine Dezentralisierung und die Verknüpfung von Informationen (Semantic Web).\nIch habe gelernt, wie die einzelnen Fachtermini LOD, Semantic Web, Web of Data und RDF in Verbindung miteinander stehen. Reflektiert betrachtet, erkenne ich, dass mir persönlich Grafiken/Mindmaps enorm helfen um Verbindungen zu erkennen und diese zu verinnerlichen.\nEbenso vermittelte die Vorlesung, wie mithilfe des Resource Description Framework (RDF) Ressourcen beschrieben und verlinkt werden können. Dabei werden Triples bestehend aus einem Subjekt, Prädikat und Objekt definiert, die über URIs identifiziert werden und eine logische Aussage über die Ressource treffen.\nDie Definition von RDF Schemata wurde nur angerissen, sodass noch kein richtiges Verständnis für diese Technologie entstehen konnte. Ich hoffe dies wird in der nächsten Vorlesung weiter behandelt, ansonsten möchte ich etwas Zeit für die Recherche außerhalb der Lehrveranstaltungen in diese Technologie investieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-3/","summary":"In der Vorlesung habe ich gelernt, wie die einzelnen Fachtermini LOD, Semantic Web, Web of Data und RDF in Verbindung miteinander stehen. Reflektiert betrachtet, erkenne ich, dass mir persönlich Grafiken/Mindmaps enorm helfen um Verbindungen zu erkennen und diese zu verinnerlichen.\nEbenso lernte ich, wie mithilfe des Resource Description Framework (RDF) Ressourcen beschrieben und verlinkt werden können. Dabei werden Triples bestehend aus einem Subjekt, Prädikat und Objekt definiert, die über URIs definiert werden und eine logische Aussage über die Ressource treffen. Die Definition von RDF Schemata wurde nur angerissen, sodass noch kein richtiges Verständnis für diese Technologie entstehen konnte. Ich hoffe dies wird in der nächsten Vorlesung weiter behandelt, ansonsten möchte ich etwas Zeit für die Recherche außerhalb der Lehrveranstaltungen in diese Technologie investieren.","title":"Vorlesung 3"},{"content":"Im Laufe des Seminars sollten wir verschiedene Lifecycles für unseren Medientypen recherchieren und die wichtigsten Schritte sowohl für Institutionen als auch im Linked Data Lifecycle herausarbeiten.\nEin effektiver und faktenbasierter Lernerfolg konnte persönlich nicht erzielt werden. Reflektierend hätte sich die Gruppe bzw. ich verstärkt auf eine Recherche konzentrieren sollen, anstatt mit “Halbwissen” die verschiedenen Aufgaben zu diskutieren. Anzumerken sei jedoch, dass die Aufgaben für mich sehr unpräzise und unkonkret formuliert wurden. Ebenso sollten Aussagen über Schritte des Linked Data Lifecycles getroffen werden, die wir bis dato nicht kennengelernt haben.\nNichtsdestotrotz nehme ich mir für die kommenden Seminar vor, erst meine Ideen und bereits bekanntes Wissen zu validieren und dieses dann mit meinen Gruppenmitgliedern zu teilen.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-2/","summary":"Im Laufe des Seminars sollten wir verschiedene Lifecycles für unseren Medientypen recherchieren und die wichtigsten Schritte sowohl für Institutionen als auch im Linked Data Lifecycle herausarbeiten.\nEin effektiver und faktenbasierter Lernerfolg konnte persönlich nicht erzielt werden. Reflektierend hätte sich die Gruppe bzw. ich verstärkt auf eine Recherche konzentrieren sollen, anstatt mit “Halbwissen” die verschiedenen Aufgaben zu diskutieren. Anzumerken sei jedoch, dass die Aufgaben für mich sehr unpräzise und unkonkret formuliert wurden.","title":"Seminar 2"},{"content":" Frage: Wofür steht das Akronym RDF und welche Rolle spielt es im Semantic Web.\nAntwort: RDF steht für Resource Description Framework und ist eine grundlegende Technologie des Semantic Webs und beschreibt logische Aussagen über Ressourcen.\nIm Rahmen dieser Vorlesung habe ich mich mit der Entwicklung von Bibliotheken auseinandergesetzt. Dabei habe ich gelernt, dass diese sich durch die Digitalisierung von reinen analogen Wissenssammlungen für einige Wenige hin zu einer Wissens- und Kommunikationsschnittstelle für die gesamte Gesellschaft entwickelt haben.\nDer größte Lerngewinn bestand im neu erlangten Verständnis für die eigentliche Motivation des Webs und die Ziele des Semantic Webs/Web 3.0. Persönlich würde ich gerne noch mehr über die Verwendung von RDF im Semantic Web und dessen Bedeutung erfahren.\nAls weniger hilfreich ist rückwirkend die häufige Verwendung des Begriffes RDF zu Beginn der Vorlesung anzumerken, da dieser nicht näher eingeführt wurde. Die Einführung erfolgte am Ende der Vorlesung in kurzer Zeit, sodass ich nicht alles verstanden habe und hier noch einmal selbst recherchieren muss.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-2/","summary":"In dieser Vorlesung habe ich gelernt, dass Bibliotheken sich durch die Digitalisierung von reinen analogen Wissenssammlungen für einige Wenige hin zu einer Wissens- und Kommunikationsschnittstelle für die gesamte Gesellschaft entwickelt haben.\nDer größte Lerngewinn bestand im neu erlangten Verständnis für die eigentliche Motivation des Webs und die Ziele des Semantic Webs/Web 3.0. Persönlich würde ich gerne noch mehr über die Verwendung von RDF im Semantic Web und dessen Bedeutung erfahren.\nLeider wurde diese grundlegende Technologie häufig am Anfang der Vorlesung verwendet, ohne dass diese präzise eingeführt wurde. Dies wurde am Ende der Vorlesung in kurzer Zeit versucht nachzuholen, sodass ich nicht nicht alles verstanden habe und hier noch einmal selbst recherchieren muss.","title":"Vorlesung 2"},{"content":"In diesem Seminar habe ich erfahren, dass ich mit meinen Befürchtungen zum Schreiben dieses Lernportfolios nicht alleine bin und bemerkt, dass viele Kommilitoninnen und Kommilitonen die gleichen bzw. ähnliche Erwartungen an die Veranstaltung haben (siehe Vorlesung 13. September).\nDes Weiteren haben wir eine Definition zu Medien erarbeitet: Ein Mittel zum Transport von Informationen, welches eine diskrete oder kontinuierliche Form aufweisen kann. Medien können dabei analog oder digital sein und verschiedene Sinne ansprechen. Ebenso können sie in mehrere Kategorien geclustert werden (siehe Wiki).\nReflektierend war ich erstaunt, dass ich noch die Definition von Medien aus dem 2. Semester des Bachelorstudiengangs beherrschte und diese mit den Erwartungen von Herrn Dr. Arndt übereinstimmte.\nDie Umfragen und die daraus resultierende Interaktion mit uns Kursteilnehmern empfand ich in beiden Lehrveranstaltungen als sehr angenehm und motivierend. Leider traten immer wieder kleine Wartepausen vor Diskussionen in der “großen Runde” auf, sodass die Veranstaltung teilweise ermüdend wirkte. Deshalb setze ich mir das Ziel, weiterhin aktiv in den folgenden Vorlesungen/Seminaren zuzuhören und zu interagieren.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/seminar/seminar-1/","summary":"In diesem Seminar habe ich erfahren, dass ich mit meinen Befürchtungen zum Schreiben dieses Lernportfolios nicht alleine bin und bemerkt, dass viele Kommilitoninnen und Kommilitonen die gleichen bzw. ähnliche Erwartungen an die Veranstaltung haben (siehe Vorlesung 13. September).\nDes Weiteren haben wir eine Definition zu Medien erarbeitet: Ein Mittel zum Transport von Informationen, welches eine diskrete oder kontinuierliche Form aufweisen kann. Medien können dabei analog oder digital sein und verschiedene Sinne ansprechen.","title":"Seminar 1"},{"content":"Ich habe gelernt, wie die Methode \u0026ldquo;Constructive Alignment\u0026rdquo; aufgebaut ist und angewendet wird. Dabei hat mich überrascht, dass Herr Dr. Arndt seine didaktische Vorgehensweise in der Vorlesung vorstellt und diese eingehend erläutert. Dies ist für mich neu und hat mich im positiven Maße beeindruckt, da so der Aufbau der Vorlesung und einige Entscheidungen nachvollziehbarer erscheinen.\nQuelle: https://medicine.uq.edu.au/files/52361/Celia-Biggs%20image.JPG\nEbenso wurde innerhalb der Vorlesung die wesentlichen Schritte des Lebenszyklus von Daten, insbesondere von medialen Daten, erklärt und besprochen. Dies ermöglichte mir einen ersten Einblick in das Management eines Lebenszyklusses.\nIch erwarte bzw. erhoffe mir von der Veranstaltung Media Lifecycle Management zunächst einen Einblick in die verschiedenen Medienarten und ihre Besonderheiten. Ebenso möchte ich gerne den gesamten Lebenszyklus näher kennenlernen und die Merkmale eines jeweiligen Schrittes vermittelt bekommen. Darüber hinaus würde ich gerne Informationen zur praktischen Relevanz und Umsetzung vermittelt bekommen. Außerdem wäre die dedizierte Erwähnung von Herausforderungen bzw. Besonderheiten der jeweiligen Medienarten für ein besseres Verständnis wünschenswert.\n","permalink":"https://fredericbahr.github.io/lernportfolio/posts/vorlesung/vorlesung-1/","summary":"Ich habe gelernt, wie die Methode \u0026ldquo;Constructive Alignment\u0026rdquo; aufgebaut ist und angewendet wird. Dabei hat mich überrascht, dass Herr Dr. Arndt seine didaktische Vorgehensweise in der Vorlesung vorstellt und diese eingehend erläutert. Dies ist für mich neu und hat mich im positiven Maße beeindruckt, da so der Aufbau der Vorlesung und einige Entscheidungen nachvollziehbarer erscheinen.\nQuelle: https://medicine.uq.edu.au/files/52361/Celia-Biggs%20image.JPG\nEbenso wurde innerhalb der Vorlesung die wesentlichen Schritte des Lebenszyklus von Daten, insbesondere von medialen Daten, erklärt und besprochen.","title":"Vorlesung 1"}]